# BertClassifier

本文利用了[transformers](https://github.com/huggingface/transformers)中的BertModel，对部分cnews数据集进行了文本分类，用来对[BERT](https://arxiv.org/abs/1810.04805)模型练手还是不错的。

<!-- more -->

## 数据描述
数据集是从清华大学的[THUCNews](http://thuctc.thunlp.org/)中提取出来的部分数据。

训练集中有5万条数据，分成了10类，每类5000条数据。
```JSON
{'体育': 5000, '娱乐': 5000, '家居': 5000, '房产': 5000, '教育': 5000, '时尚': 5000, '时政': 5000, '游戏': 5000, '科技': 5000, '财经': 5000}
```

验证集中有5000条数据，每类500条数据。
```JSON
{'体育': 500, '娱乐': 500, '家居': 500, '房产': 500, '教育': 500, '时尚': 500, '时政': 500, '游戏': 500, '科技': 500, '财经': 500}
```

## 模型描述
整个分类模型首先把句子输入到Bert预训练模型，然后将句子的embedding输入给一个全连接层，最后把全连接层的输出输入到softmax中。

## 环境


|  硬件环境 |
|  ----  | ----  |
| GPU  | GTX1080 |
| RAM  | 64G |
s
|  软件环境 |
|  ----  | ----  |
| OS | Ubuntu 18.04 LTS |
| PyTorch | 1.6.0 |
| transformers | 3.2.0 |


如果需要数据集和BERT模型，请与我联系.